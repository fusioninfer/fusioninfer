apiVersion: fusioninfer.io/v1alpha1
kind: InferenceService
metadata:
  name: llama3-inference-service
  namespace: e2e-test
spec:
  roles:
    - name: worker
      componentType: worker
      replicas: 1
      template:
        spec:
          containers:
            - name: vllm-sim
              image: ghcr.io/llm-d/llm-d-inference-sim:latest
              imagePullPolicy: IfNotPresent
              args:
                - --model
                - meta-llama/Llama-3.1-8B-Instruct
                - --port
                - "8000"
                - --max-loras
                - "2"
                - --lora-modules
                - '{"name": "food-review-1"}'
              env:
                - name: POD_NAME
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: metadata.name
                - name: POD_NAMESPACE
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: metadata.namespace
              ports:
                - containerPort: 8000
                  name: http
                  protocol: TCP
              resources:
                requests:
                  cpu: 100m
                  memory: 128Mi
                limits:
                  cpu: 500m
                  memory: 512Mi
